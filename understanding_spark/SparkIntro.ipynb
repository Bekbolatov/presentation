{"cells":[{"cell_type":"code","source":["# Load Newsgroups dataset from Databricks\nnews20 = spark.read.parquet(\"dbfs:/databricks-datasets/news20.binary/data-001/training\")\n# Cache data in memory\nnews20.cache()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Show a few elements\nnews20.show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Counts number of elements in each topic\nnews20.groupBy('topic').count().show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Count number of elements with positive and negative labels\nnews20.groupBy('label').count().show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# For this demo let us discard spam messages (using '< 0.5', for good practice in dealing with double values)\nnews20 = news20.where(news20['label'] < 0.5)\nnews20.cache()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Add a new column 'text_length' that contains the length of data in column 'text'\nfrom pyspark.sql.functions import length\nnews20_with_lengths = news20.withColumn('text_length', length(news20['text']))\nnews20_with_lengths.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Find average length of text in each topic\nnews20_with_lengths.groupBy('topic').avg('text_length').orderBy('avg(text_length)').show(truncate = False)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Create an RDD of texts\ntopic_name = 'misc.forsale'\n# topic_name = 'rec.sport.baseball'\nraw_texts = news20.filter(news20['topic'] == topic_name).select('text').rdd.map(lambda x: x[0])\n\n# Separate out the body of the email (remove mail headers)\ntexts = raw_texts.map(lambda x: \" \".join(x.split(\"  \")[1:]))\ntexts.take(1)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Split into words (flatMap)\nwords = texts.flatMap(lambda text: text.split())\nprint(words.take(7))\n\n# Make words lowercase\nwords = words.map(lambda word: word.lower())\nprint(words.take(7))\n\n# Filter out words that only have letters\nwords = words.filter(lambda word: word.isalpha())\nprint(words.take(7))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Load stop words from a URL\nimport urllib2\nstopwords_data = urllib2.urlopen('https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/data/edu/stanford/nlp/patterns/surface/stopwords.txt')\nstopwords = [line[:-1].lower() for line in stopwords_data]\nstopwords[40:50]\n\n# Filter out those words:\nwords = words.filter(lambda p: p not in stopwords)\nprint(words.take(7))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Create pairs as (word, 1)\nwords_with_one = words.map(lambda word: (word, 1))\nprint(words_with_one.take(7))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from operator import add\nprint(topic_name)\nwords_with_one.reduceByKey(add).takeOrdered(20, key=lambda x: -x[1])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["print(topic_name)\nwords_with_one.reduceByKey(add).takeOrdered(20, key=lambda x: -x[1])"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"SparkIntro","notebookId":4167717899070060},"nbformat":4,"nbformat_minor":0}
